{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4pky5FP7yv0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoC4tsak8MQv",
        "outputId": "c375caec-4aec-4002-afa3-a0a7a8595eca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is a sample text for data preprocessing. It contains stop words and words that need stemming.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pmevl3658RDQ",
        "outputId": "ea0acae6-ce4b-40ca-9ae0-8a7dd49d77a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['This', 'is', 'a', 'sample', 'text', 'for', 'data', 'preprocessing', '.', 'It', 'contains', 'stop', 'words', 'and', 'words', 'that', 'need', 'stemming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "normalized_tokens = [token.lower() for token in tokens]\n",
        "print(\"Normalized tokens:\", normalized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iD9ht2C8cQF",
        "outputId": "80ebeb43-9130-409a-b681-b665f580ce06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized tokens: ['this', 'is', 'a', 'sample', 'text', 'for', 'data', 'preprocessing', '.', 'it', 'contains', 'stop', 'words', 'and', 'words', 'that', 'need', 'stemming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in normalized_tokens]\n",
        "print(\"Stemmed tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UD1EY-k98ilo",
        "outputId": "805f81f8-aabb-4669-cc24-b8a05b19c1bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed tokens: ['thi', 'is', 'a', 'sampl', 'text', 'for', 'data', 'preprocess', '.', 'it', 'contain', 'stop', 'word', 'and', 'word', 'that', 'need', 'stem', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removal of Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [token for token in stemmed_tokens if token not in stop_words]\n",
        "print(\"Filtered tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cBYp5ZD8nEq",
        "outputId": "52fa28d6-ff3b-4443-b12e-9e99ae94d064"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tokens: ['thi', 'sampl', 'text', 'data', 'preprocess', '.', 'contain', 'stop', 'word', 'word', 'need', 'stem', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k3LbIDV-8rm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}